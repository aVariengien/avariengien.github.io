---
layout: post
title: "Scaling what used not to scale"
subtitle: "Using fluid intelligence to scale intellectual progress without systems."
mathjax: true
excerpt: "(Part 2/2 of my introduction to Live Theory) In the first part, I described systematic thinking as the most successful coordination tool of our civilization."
hide_from_list: false
permalink: scaling-what-used-not-to-scale
thumbnail-img: /assets/img/scaling-what-used-not-to-scale/image-0.png
---

​*This is part 2/2 of my introduction to [Live Theory](https://www.lesswrong.com/s/aMz2JMvgXrLBkq4h3), where I try to distil [Sahil’s](https://www.lesswrong.com/users/sahil-1) vision for a new way to scale intellectual progress without systematic thinking.*

In the first part, I described systematic thinking as the most successful coordination tool of our civilization. It’s the infrastructure that allowed billions of brains to work as a collective brain, solving problems no society could solve before. I finished by introducing Live Theory as a successor to systematic thinking. This is a big claim! In this part, I will finally unpack (my best guess of) what Live Theory is, and how it can claim to go beyond systematic thinking, fixing its inability to deal with context-dependent problems.

### **The three-picture summary.**

**Pre-systematic thinking is playdough.** *Examples of outputs: Poetry coming from your heart, a tailor-made shirt made by your grandma.*

Pre-systematic thoughts are fluid and include verbal and non-verbal modalities. They can bend and wrap around the real-world objects they encounter. The conceptual and physical artefacts it produces are wiggly; they cannot easily interface with one another.

{% include image.html src="/../assets/img/scaling-what-used-not-to-scale/image-0.png" alt="" caption="" width="80%" %}

​

**Systematic thinking is a brick mould for thought.** *Examples of outputs: physics equations, software bundled and distributed to billions of users, H&M pants with different sizes that can somewhat work for everyone, but are not made for a specific body shape.*

{% include image.html src="/../assets/img/scaling-what-used-not-to-scale/image-1.jpg" alt="" caption="" width="80%" %}

The thoughts are unfolding within a system, a set of rules describing allowed and non-allowed moves. Systems act as moulds for thoughts. They turn the fluid content of the mind into bricks that can be stacked on top of each other. This brought two major benefits:

1. Stacking thoughts within one’s mind to build deeper, more complex thoughts.
2. Stacking thoughts across people, so you can use the conclusions found by strangers who use the same formal system as you do.

**The Live Theory vision is to use abundant fluid intelligence as a mortar that adapts to intellectual contributions of all shapes and connects them.** *Example of ~~outputs~~. Well, the notion of “output” doesn’t make much sense anymore; more on that later. But an aspirational example could be: a well-chosen piece of poetry and math equations being used as input by an AI infrastructure to write an algorithm to solve a* market-matching *problem.*

{% include image.html src="/../assets/img/scaling-what-used-not-to-scale/image-2.jpg" alt="" caption="" width="80%" %}

In this vision, AI is used to create an infrastructure that adapts to the shape of the individual contribution. The infrastructure can translate insights from systematic, pre-systematic or even post-systematic sources, to tailor them to new contexts.

### **Short recap from Part I.**

Pre-systematic artefacts are impregnated by the context in which they are forged: the specific tools used, the specific know-how found by a master. It takes direct mentorship or apprenticeship to convey, which slows intellectual progress.

The stackable thoughts from systematic thinking are the innovation that connected millions of human brains into a single high-bandwidth cognitive system, where results found at one side of the world could be effortlessly reused by everyone. This is the cognitive infrastructure that is behind all of our modern institutions: financial systems, globalized markets, scientific method, legal systems, etc.

However, the thoughts can only fit the world to the extent that the system is a good proxy for the world. And because the system is meant to scale to the whole world, its solutions fit moderately well for everyone. Even with tunable parameters, systems are too rigid to truly adapt to the different application contexts.

The scale from systems was so successful that we became addicted to it. To keep it going, we shaped the world to remove context-specific idiosyncrasies so that systematic solutions would apply better across contexts. It’s like doing plastic surgery to change your body shape so the H&M pants would fit you perfectly.

However, the systematic infrastructure is fundamentally ill-suited to problems that are context-sensitive. No big deal, these are just the most important problems of our time, like aligning technological development to human flourishing. By default, the development of AI will only amplify our systematic machine, making these problems worse, like forcing a square into a round-shaped hole.

### **Some context on Live Theory.**

**A note on terminology.** I use “Live Theory” (capitalized) for the name of the whole vision, shortened to “the live vision” for brevity. It is a hypothetical world in which our infrastructure and artefacts are “fluid” or “live”. I say “*a* live theory”, to talk about a new kind of artefact that replaces the classic “theory” from the systematic world.

**What is it not?** Live Theory is not a new “framework”, “protocol” or “method” to coordinate at scale. That’s the whole point! Otherwise, this would be designing a good old global system, exactly the thing we want to go beyond. At the moment, Live Theory barely exist. It is a quiet whisper from the future, suggesting that the near-term abundant fluid intelligence might enable a radically different way to scale intellectual progress.

It is not “add an AI chatbot to X”. Even if the AI infrastructure play an important role in the story, whenever you see AI, think in your head “Google Translate, but for everything: [image, video, audio, code, math, text in all languages] to [image, video, audio, code, math, text in all languages]” and not “ChatGPT”.

**Who is working on it?** This blurry vision is slowly becoming clearer through the work of Sahil, its initiator, and multiple co-thinkers and collaborators working to build prototypes and clarify concepts through debate. You can learn more about their research on [the groundless website](https://groundless.ai/).

The bottleneck of this work at the moment is imagination. To imagine something outside the systematic frame requires unlearning the mental muscle memory from decades of systematic thinking. Many projects from the groundless team take the form of interface prototypes that aim to concretely embed this new paradigm for compounding intellectual contributions.

### **Post-rigorous mathematics**

If systematic thinking is so sticky that the hardest part of building the vision is getting out of it, it is a good idea to start by looking at today’s examples of things that are not systematic thinking.

We can find such examples in people working close to the systematic fire: mathematicians. Their job is to produce papers filled with symbols, formulas and proofs. However, professional mathematicians rarely *think* in symbols. Their language is rich in intuitive statements like “high-dimensional spheres are [spiky](https://www.solipsys.co.uk/new/SpikeySpheres.html)“. But when they need to try an intuition for real, they can effortlessly translate these thoughts into rigorous math.

I once had the chance to attend a lecture given by the Fields Medallist Jean Paul Serre. I didn’t understand much of the content, but I remember vividly how he effortlessly sprinkled equations on the whiteboard, filling the in-between with imagined descriptions, what you would call now “vibes”. He looked like a blacksmith simply laying his sharpest tools on the workstation for display, trying to convey, with imperfect words, the embodied know-how living in his hands.

This stage of reasoning is called [post-rigorous](https://terrytao.wordpress.com/career-advice/theres-more-to-mathematics-than-rigour-and-proofs/) (that I will refer to here as post-systematic thinking). The rigour serves as the foundation on which the inner world of a mathematician grows, rich in intuitions, imagery, and, believe it or not, emotions. Over years of practice, from encountering problems in different fields, mathematicians create their own language of thoughts that pre-exists the rigorous proofs.

Even if it constitutes the bulk of the mathematician’s lived experience, this world rarely gets shared. The squishy, subjective experience doesn’t belong in conference papers. A rigorous proof is the final product of a mathematician’s work, even if post-rigour is what produced it.

The vast world of mathematical intuitions developed through years of practice is shared in the old way: through apprenticeship from PhD supervisors to PhD students.​

From the live theory perspective, this is an immense missed opportunity. If only we could distribute these insights as widely as a mathematical proof!

### **A translation infrastructure built from abundant fluid intelligence.**

In the past decade, we created computer programs that can interact with natural language, create music, video and images. Where software of the past was restricted to the automatic application of systematic rules, we now have software that demonstrates a fluidity that normally characterizes pre- or post-systematic thinking.

This new kind of software reduces the cost of certain fluid tasks by orders of magnitude, such as high-quality translation or solving programming problems. The hypothesis of live vision is that AI will continue to develop in a manner similar to that of computers or the Internet. It will become a more reliable, cheaper and faster version of the multimodal LLM we have today, but without the ability to generate revolutionary results.​

For the live vision, the killer application of AI will not be the generation of new ideas from scratch; it will be interpretative labour, translating insights from one context to another. The idea is to use this abundant fluid intelligence as a mortar between intellectual contributions, whatever their shape. In systematic thinking, your intellectual contribution has value only if it fits the brick mould used by everyone else. In a live world, your intellectual contribution has value, full stop. No matter if your insight is expressed as poetry, equations, a drawing, a book, or an audio recording of a conversation, the content can be cheaply translated to whatever shape would be useful for the consumer at the last minute.

Instead of forcing the output of intellectual labour to fit within the systematic infrastructure, a fluid infrastructure adapts to the shape of the intellectual contributions to distribute them.

**Depth matters.** This is not to say that any squiggle on a whiteboard can be turned into the equations of general relativity. The squiggle was simply an arbitrary gesture I made with a pen. It contains insights about my handwriting, the texture of the whiteboard, maybe how agitated I was when I drew it or what I was thinking. Even with all the translations in the world, the squiggle’s *potential reach* is limited to its immediate context. At most, I could use it to understand my emotional state at a specific moment.

To contrast, take the squiggles in the notebook of Einstein before he finalized the equation of relativity. There is also a lot of context dependency in these handwritten notes. He probably used a ton of *ad hoc* abbreviations. But there is more to it. The notes contain a depth of understanding of physics that will eventually lead to the most important equations of modern physics. It is fair to say that a big part of the insights was likely already present well before the final equations were written. But from the systematic point of view, the values of the notes are almost zero until they contain usable formulas. However, the *potential reach* of Einstein’s notebook after interpretation is enormous. They have the potential to change the worldview of thousands of physicists and explain observations from the other side of the universe.

The depth of the intellectual labour matters, even if the fluid infrastructure can adapt to its form. Systematic thinking will keep playing an important role in a live world, because systems are wonderful tools to build complex thoughts. But at last, the depths of insights coming from mathematical equations would be able to interact with the depth coming from other domains, from physics, biology, philosophy, or poesy, or the post-systematic insights from my lecture from Jean Paul Serre!​

**Fluid book generation.** To concretise these ideas, let’s look at what writing and reading a book could look like in a live world. (Example taken from this [table of live examples](https://www.lesswrong.com/s/aMz2JMvgXrLBkq4h3/p/9KamjXbTaQpPnNsxp#Preface_for_the_table_).)

The writer writes not a book, but a book-prompt that carries intuitions for the insights. An economics textbook-prompt, for example, could contain pointers to the results and ideas but the language, examples, style would combine with readers’ backgrounds (and specific wishes) to turn into independent textbooks for each reader. Importantly, this frees up producers of books from having to homogenize the range of audiences they might want to speak to.

This is a bit like changing the font and the colour of an e-book, but for content. Specific wishes the readers could have (such as “I’m looking for something that would make me laugh”) are things that could be appended to the original book-prompt. They don’t have to be independent books either, and can be *inter*dependent. Commentary on your own book can be translated to someone else’s book.

**Value moves upwards.** As mentioned in the intro, with a fluid infrastructure, the distinction between input and output blurs. Before you had a book, a static output from the writing process, which is the input to the reading process. In a live world, the final book the reader sees is co-created by her own wishes, background information (maybe as a list of books she knows really well), and the writer’s book-prompt.​

The general tendency is that potentials become first-class citizens, like the book-prompt that is a potential for a book. In our world, the execution work that goes into turning a draft into a polished book or a drawing into software is expensive. But in a live world, this work is as cheap as water or electricity. So the value moved upstream: the draft itself becomes valuable, and even before the draft, the recording of the conversations in which the ideas first appear becomes valuable.

To be extra clear, this doesn’t mean AI slop generated from vague prompts. Depth and mastery from any craft will be more valued than ever. But the artefacts people produce with this depth will change.

To recap, fluid intelligence might make the basis for a fluid infrastructure that could scale intellectual progress without forcing the shape of the outputs. We left the part I with all sorts of problems stemming from the inability of systematic thinking to adapt to context, such as aligning technological development to human flourishing. How does this vision for a fluid infrastructure address these?

### **The system-world fit is the Achilles’ heel of systematic thinking.**

The rigidity of institutions based on systematic thinking comes from an imperfect fit between the system they use and the world they interact with. The circle arrow in the diagram below is the Achilles’ heel of systematic thinking.

​

{% include image.html src="/../assets/img/scaling-what-used-not-to-scale/image-3.jpg" alt="" caption="" width="80%" %}

​

No matter how much work is put into producing new results within a system, the body of knowledge produced is bounded by how well the system fits the world. In this sense, systems are static; they cannot dynamically evolve to keep pace with the changing form of the world.

**Lock-in effect from globalized systems.** Systems are often seen as non-overlapping *spaces*. An algorithm is either written in Rust or in Python. It cannot be in both at the same time, like how an apple cannot be in my house and in my neighbour’s house at the same time.

Imagine you want to change the fit system-world to better match your specific context. Maybe you want to design a new programming language adapted to your new hardware, or a new legal system adapted to your country. Then, you have to start from scratch. You will need a lot of work to translate the insights from other systems into your new system.

This creates a centralization dynamic where the biggest systems are likely to grow even bigger. It is the network effect well known by the giant platforms of our digital world. You need to install this new app to chat with your friends. You’d like to use your niche app with this great interface that fits your habits, but you’d have no one to talk to, so you bite the bullet and install the new app.

This means that while we can easily iterate within a system to find a better solution, we cannot iterate on the system-world fit.

**There is no market to improve the system-world fit.** Because designing new systems is so expensive, the fit of our system doesn’t improve over time. They keep producing these globally distributed solutions that fit moderately well. The solutions don’t adapt to anyone’s context unless you change your context to fit the solution.

From the live perspective, the root of many risks arising from technological development is this system lock-in and the poor, static system-world fit they impose globally.​

To solve these problems, the solution needs to adapt to their context. No matter how many parameters it contains, no systematic framework can be used to align technology, or AI, with humanity flourishing. Moreover, as technology evolves, what “alignment” means needs to be continuously renegotiated as quickly as the underlying technology evolves. In the future, the rate of change in the underlying reality might be so rapid that the fit between the system and the solution could break down rapidly. It would be impossible for systematic thinking to keep up by designing new systems on the fly.

**Diffuse concepts for a diffuse response to diffuse risks.**

The live vision answer is to leverage the fluid infrastructure to create *live theories*, the equivalent of systematic theory, but that incorporate post-systematic insights. The fluid infrastructure would continuously renegotiate the fit between the live theory and the world as the underlying reality changes.

Intellectual labour would stop being exchanged on disjoint markets with solutions valid only within a system. It would be distributed through a peer-to-peer network for insight sharing, where new theory-world fits could be developed in one place and applied in another context.​

Here is a speculative story to illustrate the argument. Imagine you are a well-intentioned developer working on a new AI assistant that has been caught lying to its users. The old way would be to follow a recipe from the scientific literature, change the way the assistant is trained, and distribute the new version to all users. A more live way to do it would be to start by gathering sources from philosophy on deception, technical machine learning resources, and maybe even sources from biology on deceptive [mimicry](https://en.wikipedia.org/wiki/Mimicry). You would then combine these sources and add your own work to make an “update prompt”. The update prompt will adapt to each user’s custom environment and continuously monitor and influence the AI assistant to steer it based on both your update prompt and the user’s preferences.

The vision is to diffuse the *concepts* (produced from intellectual labour), to diffuse the response (the context-specific application of the concepts), and to diffuse the risks (from technological development). This is the one-line vision you can find on the [groundless.ai](https://groundless.ai/) website.

### **Where is the theory-world fit created?**

If the theory-world *fit* is the undersupplied ingredient that live theory’s peer-to-peer network tries to distribute, where does it come from? Why can’t AI automate the creation of this magical fit?​

A full answer is well outside the scope of this post, but I find it important to offer some response, as they are load-bearing hypotheses for the live theory vision.

​It is unclear how exactly humans come to develop subjective experiences that *fit* (for lack of a better word) the world, but this might have to do with their embodiment.​

Humans constantly care for their survival at all scales, from fighting pathogens to creating social bonds to build mutual aid networks. This sort of “skin in the game” might be an important factor in how this fit gets developed.

​In the medium term, AI might struggle to generate insights that are well-fitted and deeply relevant to the world because of its lack of embodiment. And, no, robots filled with sensors will not be enough. Biological beings implement a form of integrated, fractal homeostasis, from psychological functions to mitochondria, that simple mechanical bodies cannot naively replicate. It’s not that it’s impossible to replicate artificially, but the current tech lacks this multi-scale integration.

### **Conclusion.**

The Live Theory vision forms a large puzzle made of pieces that seem unrelated but are in fact supporting the weight of each other. It has been a difficult task to carve out a coherent story from these pieces. My central goal was to describe the *function* Live Theory aims at filling and how it differs from its predecessor, systematic thinking. Here are a few topics I left aside:​

* A different view on catastrophic AI risks: “AI will fail at self-preservation, not because they will fail at preservation but because they will fail at self”. Maintaining the integrity of a porous boundary around the “self” is a very hard challenge. See more [here.](https://docs.google.com/document/d/1Ncz2QxS9d5q5-FXRdtPILmVJp7DGGOdy_jjLWHvZO2U/edit?tab=t.0)
* Why is most of the concrete work of the live theory folk focused on interface design?
* In a world with fluid infrastructure, what remains rigid?
* Why is the root of the risks from technological development the lack of context-sensitivity of systems?
* What are the dangers of an intelligence scarcity mindset (from systematic thinking) applied in an abundant world?

If you are interested in learning more, I would recommend the [Live Theory LessWrong sequence](https://www.lesswrong.com/s/aMz2JMvgXrLBkq4h3) for a deep dive, and these three [intro documents](https://groundless.ai/aisc) written for the AI Safety Camp as a shorter introduction.​
